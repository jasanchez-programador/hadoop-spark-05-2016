# Desde HIVE creamos tabla en formato parquet
hive> create table t1_parquet(id int, name string)
	  stored as parquet;

hive> insert into table t1_parquet
      select * from t1;

# Copiamos fichero configuracion de hive a spark para encontrar metastore
$ sudo cp /etc/hive/conf/hive-site.xml /etc/spark/conf

# Nos conectamos a pyspark y creamos el DataFrame

ANTES:
	[1] df_t1_parquet = sqlCtx.parquetFile("/user/hive/warehouse/t1_parquet")
AHORA
	[1] df_t1_parquet = sqlContext.read.parquet('/user/hive/warehouse/t1_parquet')

# guardamos como json en HDFS
ANTES
	[1] df_t1_parquet.save('/user/hive/warehouse/t1_json','json')
AHORA
	[1] df_t1_parquet.write.json('/user/hive/warehouse/t1_json')
